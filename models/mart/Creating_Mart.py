# -*- coding: utf-8 -*-

"""

Copy of NeoBank_Customer_Engagement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ci68_JKwWi8hTGtOhJr2a0Alrukj5AuM

IMPORTING DATA FROM GBQ - IMPORTING LIBRARIES

"""

from google.colab import auth
auth.authenticate_user()

import os
os.environ["GOOGLE_CLOUD_PROJECT"] = "neonauts"

from google.cloud import bigquery
client = bigquery.Client()

import pandas as pd


query = "SELECT * FROM `neonauts.dbt_azoellner.int_transformed_neo_bank`"

df = client.query(query).to_dataframe()

date_columns = [
    "sign_up_date",
    "first_notification",
    "last_notification",
    "first_transaction_date",
    "last_transaction_date"
]

for col in date_columns:
    df[col] = pd.to_datetime(df[col], errors='coerce')


""" MACHINE LEARNING FOR COEFFICIENTS """

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

ML_scaler = StandardScaler().set_output(transform="pandas")
model= LogisticRegression()

X= df[[
    "avg_transactions_per_day",
    "crypto_unlocked",
    "is_standard_user",
    "is_premium_user",
    "is_metal_user",
    "average_amount_per_transaction_usd",
    "active_timeframe",
    "apple_user",
    "android_user",
    "age",
    "nb_notifications",
    "direction_ratio"
]]

y= df["active_user"]

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state= 42)

X_train_scaled = ML_scaler.fit_transform(X_train)
X_test_scaled = ML_scaler.transform(X_test)

model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
from sklearn.metrics import accuracy_score,recall_score,roc_auc_score

print("Accuracy (Test):", accuracy_score(y_test, y_pred))
print("Recall (Test):", recall_score(y_test, y_pred))

coefficients = model.coef_[0]
columns = X.columns

df_coefficients = pd.DataFrame({
    'Feature': columns,
    'Coefficient': coefficients
})

df_coefficients = df_coefficients.sort_values(by='Coefficient', ascending=False).reset_index(drop=True)

"""
WEIGHTING FOR LES
Hybrid Approach: Use the coefficients of the ML-Model as a foundation for the model weighting, but adjust based on their relevance for product strategy.
"""

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Engagement-Features

engagement_features = [
    "avg_transactions_per_day",
    "crypto_unlocked",
    "is_standard_user",
    "is_premium_user",
    "is_metal_user",
    "average_amount_per_transaction_usd",
    "active_timeframe",
    "direction_ratio"
]

# Scaling the features

scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df[engagement_features]), columns=engagement_features)


# Setting the weighting for the engagement score

weights = {
    "active_timeframe": 45,
    "avg_transactions_per_day": 20,
    "direction_ratio": 15,
    "is_metal_user": 7,
    "is_premium_user": 8,
    "is_standard_user": 0,
    "average_amount_per_transaction_usd": 3,
    "crypto_unlocked": 2
}

# calculating LES

df_scaled["LES"] = sum(
    df_scaled[feature] * weight for feature, weight in weights.items()
)

# adding the user_id

df_scaled["user_id"] = df["user_id"]


""" SEGMENT CLUSTERING WITH KMEANS """

import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Prepare the data
X = df_scaled[["LES"]].copy()

# 2. Apply KMeans with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
df_scaled['segment_kmeans'] = kmeans.fit_predict(X)

# 3. Sort clusters by average score for interpretability
cluster_order = df_scaled.groupby('segment_kmeans')['LES'].mean().sort_values().index

# 4. Map cluster labels to meaningful names
segment_labels = {cluster_order[0]: 'Low Engagement',
                  cluster_order[1]: 'Medium Engagement',
                  cluster_order[2]: 'High Engagement'}
df_scaled['segment_label'] = df_scaled['segment_kmeans'].map(segment_labels)

""" CREATING THE MART TABLE """

df_LES_segment = df_scaled[["user_id", "LES", "segment_label"]]
df_scaled["user_id"] = df_scaled["user_id"].astype(int)
df["user_id"] = df["user_id"].astype(int)

mart_user_LES_Neo_Bank = df_scaled[["user_id", "LES", "segment_label"]].merge(
    df,
    on="user_id",
    how="left"
)

# 1. extracting the same features as we had in the original ML-model
X_full_data = df[[
    "avg_transactions_per_day",
    "crypto_unlocked",
    "is_standard_user",
    "is_premium_user",
    "is_metal_user",
    "average_amount_per_transaction_usd",
    "active_timeframe",
    "apple_user",
    "android_user",
    "age",
    "nb_notifications",
    "direction_ratio"
]]

# 2. scaling with the same Scaler we used for the original ML-Model
X_full_data_scaled = ML_scaler.transform(X_full_data)

# 3. Calculating the probabilitys (0 = churn/ inactive)
probabilities = model.predict_proba(X_full_data_scaled)
churn_probability = probabilities[:, 0]

# 4. creating a DF with user ID and churn-probability
df_churn_prediction = pd.DataFrame({
    'user_id': df['user_id'],
    'churn_probability': churn_probability
})

mart_user_LES_Neo_Bank['user_id'] = mart_user_LES_Neo_Bank['user_id'].astype(int)
df_churn_prediction['user_id'] = df_churn_prediction['user_id'].astype(int)

mart_user_LES_Neo_Bank = mart_user_LES_Neo_Bank.merge(
    df_churn_prediction,
    on="user_id",
    how="left"
)

""" EXPORT TO BIGQUERY """

table_id = "neonauts.dbt_maxstuenzner.mart_user_LES_Neo_Bank"

job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")

job = client.load_table_from_dataframe(mart_user_LES_Neo_Bank, table_id, job_config=job_config)

job.result()

print("Table created successfully:", table_id)
